{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from __future__ import division\n",
    "import traceback\n",
    "\n",
    "# with dense vector negative sample 0, postive sample 1\n",
    "def performance(f):\n",
    "    def fn(*args, **kw):\n",
    "        t_start = time.time()\n",
    "        r = f(*args, **kw)\n",
    "        t_end = time.time()\n",
    "        print ('call %s() in %fs' % (f.__name__, (t_end - t_start)))\n",
    "        return r\n",
    "    return fn\n",
    "\n",
    "\n",
    "# U is 2d-array with 2m*d dimension for each person item tuple of (x, label)\n",
    "\n",
    "def mlr(W, U, x):\n",
    "    \"\"\"\n",
    "    calculate mixture logistic regression\n",
    "    :param U: m * d\n",
    "    :param W: m * d\n",
    "    :param x: d\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    ux = np.dot(U, x)\n",
    "    eux = softmax(ux)\n",
    "    del ux\n",
    "    return np.dot(eux, sigmoid(np.dot(W, x)))\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    calculate sigmoid\n",
    "    :param z:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    z = z.clip(-10, 10)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    softmax a array\n",
    "    :param x:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    e_x = np.exp(x)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def calLoss(X, y, weight_W, weight_U, norm21, norm1):\n",
    "    \"\"\"\n",
    "        计算loss\n",
    "    :param data:\n",
    "    :param weight_W:\n",
    "    :param weight_U:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #混合逻辑回归的loss\n",
    "    functionLoss = calFunctionLoss(weight_W, weight_U, X, y)\n",
    "    #L21正则的loss\n",
    "    norm21Loss = calNorm21(weight_W + weight_U)\n",
    "    #L1正则的loss\n",
    "    norm1Loss = calNorm1(weight_W + weight_U)\n",
    "    print(functionLoss , norm21 * norm21Loss , norm1 * norm1Loss)\n",
    "    return functionLoss + norm21 * norm21Loss + norm1 * norm1Loss\n",
    "\n",
    "def calFunctionLossOne(W_w, W_u,x, y):\n",
    "    p = mlr(W_w, W_u, x)\n",
    "    if y == 0:\n",
    "        return - np.log(1 - p)\n",
    "    else:\n",
    "        return - np.log(p)\n",
    "\n",
    "\n",
    "def calFunctionLoss(W_w, W_u, X, y):\n",
    "    \"\"\"\n",
    "    calculate the loss over all data\n",
    "    :param w_w:\n",
    "    :param w_u:\n",
    "    :param data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    loss = map(lambda (x,y): calFunctionLossOne(W_w,W_u,x, y), zip(X, y))\n",
    "    loss = sum(loss)\n",
    "    return loss\n",
    "    # print(\"loss is:  %s\" % loss)\n",
    "\n",
    "def calNorm21(weight):\n",
    "    '''\n",
    "        计算norm21\n",
    "    :param weight:\n",
    "    :return:\n",
    "    '''\n",
    "    return (weight ** 2).sum() ** 0.5\n",
    "\n",
    "def calNorm1(weight):\n",
    "    \"\"\"\n",
    "        计算norm1\n",
    "    :param weight:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return np.abs(weight).sum()\n",
    "\n",
    "def calDimension21(W):\n",
    "    \"\"\"\n",
    "        计算每一个维度的L2\n",
    "    :param W:\n",
    "    :return:{dimension1:std1, dimension2:std2 ......}\n",
    "    \"\"\"\n",
    "    return (W**2).sum(axis = 0) ** 0.5\n",
    "\n",
    "# derivative for each sample\n",
    "def cal_derivative(W_w, W_u, x, y):\n",
    "    \"\"\"\n",
    "    calculate derivative\n",
    "    :param weight:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    ux = np.dot(W_u, x)\n",
    "    eux = softmax(ux)\n",
    "    del ux\n",
    "    sig = sigmoid(np.dot(W_w, x))\n",
    "    mlr = np.dot(eux, sig)\n",
    "    prob_scalar =  - (y - mlr) / (mlr * (1 - mlr))\n",
    "    dir_U = np.outer(prob_scalar * eux * (sig - mlr), x)\n",
    "    dir_W = np.outer(prob_scalar * sig * (1 - sig) * eux, x)\n",
    "    return dir_W, dir_U\n",
    "\n",
    "\n",
    "def sumCalDerivative(WW, WU, X, y):\n",
    "    all = map(lambda (x,y): cal_derivative(WW, WU,x, y), zip(X,y))\n",
    "    LW, LU = reduce(lambda x, y: (x[0] + y[0], x[1] + y[1]),all,(0,0))\n",
    "    return LW, LU\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def virtualGradient(WW, WU, GW, GU,beta,lamb):\n",
    "    \"\"\"\n",
    "    :param weight_W:\n",
    "    :param weight_U:\n",
    "    :param gradient_W:\n",
    "    :param gradient_U:\n",
    "    :param norm21:\n",
    "    :param norm1:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #计算θ_i·\n",
    "    D21 = calDimension21(WW + WU)\n",
    "    #计算v：\n",
    "    VW = calV(GW, beta)\n",
    "    VU = calV(GU, beta)\n",
    "    #计算v_i·\n",
    "    VD21 = calDimension21(VW + VU)\n",
    "    sumVD21 = sum(VD21)\n",
    "    #计算d_ij\n",
    "    DW = calDij(GW, WW, VW, D21, sumVD21, beta, lamb)\n",
    "    DU = calDij(GU, WU, VU, D21, sumVD21, beta, lamb)\n",
    "    return DW, DU\n",
    "\n",
    "\n",
    "def calV(L, beta):\n",
    "    \"\"\"\n",
    "        计算v，包括wv， uv，这里是分别计算的\n",
    "        （可以和到一起算，因为w，u一直都是分着算的，所以这里也分着算了。重构的时候再优化吧）\n",
    "    :param LW:\n",
    "    :param LU:\n",
    "    :param beta:\n",
    "    :param lamb:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    V = np.copy(L)\n",
    "    V = np.maximum(np.abs(V) - beta, 0)\n",
    "    return V*np.sign(-L)\n",
    "\n",
    "def calDij(L, W, V, D21, sumVD21, beta, lamb):\n",
    "    mask1 = (W != 0)\n",
    "    mask2 = (W == 0) * np.tile((D21 != 0), (len(W),1))\n",
    "    mask3 = np.tile((D21 == 0), (len(W),1))\n",
    "    D21_tmp = np.copy(D21)\n",
    "    D21_tmp[D21_tmp == 0] = 1\n",
    "    s = - L - lamb * W / D21_tmp\n",
    "    cond1 =  s - beta * np.sign(W)\n",
    "    cond2 = np.maximum(np.abs(s) - beta, 0)*np.sign(s)\n",
    "    cond3 = V * max(sumVD21 - lamb, 0) / sumVD21\n",
    "    return mask1 * cond1 + mask2 * cond2 + mask3 * cond3\n",
    "\n",
    "\n",
    "def loop(length, latest, direction):\n",
    "    count = 0\n",
    "    if(latest >= length):\n",
    "        raise Exception(\"start should be less than length\")\n",
    "    if(direction == 'right'):\n",
    "        while(count < length):\n",
    "            if(latest + count + 1 < length):\n",
    "                yield latest + count + 1\n",
    "                count += 1\n",
    "            else:\n",
    "                yield latest + count + 1 - length\n",
    "                count += 1\n",
    "    elif(direction == 'left'):\n",
    "        while(count < length):\n",
    "            if(latest - count >= 0):\n",
    "                yield latest - count\n",
    "                count += 1\n",
    "            else:\n",
    "                yield length + latest - count\n",
    "                count += 1\n",
    "    else:\n",
    "        raise Exception(\"please enter left or right\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## weight_w, weight_u, s\n",
    "def lbfgs(VW, VU, sList_w,sList_u, yList_w, yList_u, k, m, start):\n",
    "    \"\"\"\n",
    "        两个循环计算下降方向,拟合Hessian矩阵的 逆H 和梯度负方向的乘积，即 -H * f'\n",
    "    :param feaNum:\n",
    "    :param gk : matrix, 2m*d\n",
    "    :param sList:3d*matrix,steps * 2m * d\n",
    "    :param yList:3d*matrix, steps * 2m * d\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if((sList_w[start] * yList_w[start] + sList_u[start] * yList_u[start]).sum() > 0 ):\n",
    "        q_u = np.copy(VU)\n",
    "        q_w = np.copy(VW)\n",
    "        # for delta\n",
    "        L = k if k <= m else m\n",
    "        alphaList = np.zeros(L)\n",
    "        roList = np.zeros(L)\n",
    "\n",
    "        for i in loop(L, start, 'left'):\n",
    "            ro = 1 / (yList_w[i] * sList_w[i] + yList_u[i] * sList_u[i]).sum()\n",
    "            alpha = ro * (sList_u[i] * q_u + sList_w[i] * q_w).sum()\n",
    "            q_u = q_u - alpha * yList_u[i]\n",
    "            q_x = q_x - alpha * yList_x[i]\n",
    "            alphaList[i] = alpha\n",
    "            roList[i] = ro\n",
    "\n",
    "        for i in loop(L,start,'right'):\n",
    "            ro = roList[i]\n",
    "            beta = ro*(yList_u[i] * q_u + yList_w[i] * q_w).sum()\n",
    "            q_u = q_u + (alphaList[i] - beta) * sList_u[i]\n",
    "            q_w = q_w + (alphaList[i] - beta) * sList_w[i]\n",
    "\n",
    "        mask_u = np.sign(q_u) * np.sign(VU) > 0\n",
    "        mask_w = np.sign(q_w) * np.sign(VW) > 0\n",
    "\n",
    "        return q_w * mask_w, q_u * mask_u\n",
    "    else:\n",
    "        return VW, VU\n",
    "\n",
    "def backTrackingLineSearch(X, y, weight_W, weight_U,norm21, norm1, pW, pU):\n",
    "    \"\"\"\n",
    "        线性搜索，得到最佳步长并更新权重\n",
    "    :param it:\n",
    "    :param oldLoss:\n",
    "    :param data:\n",
    "    :param WW:\n",
    "    :param WU:\n",
    "    :param GW:\n",
    "    :param GU:\n",
    "    :param vGW:\n",
    "    :param vGU:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    alpha = 1.0\n",
    "    c = 0.5\n",
    "    tao = 0.9\n",
    "    LW, LU = sumCalDerivative(weight_W, weight_U, X, y)\n",
    "    m = (pW * LW + pU * LU).sum()\n",
    "    t = - c * m\n",
    "    loss = calLoss(X, y, weight_W, weight_U, norm21, norm1)\n",
    "\n",
    "    while True:\n",
    "        newW = weight_W - alpha*pW\n",
    "        newU = weight_U - alpha*pU\n",
    "\n",
    "        new_loss = calLoss(X, y, newW, newU, norm21, norm1)\n",
    "\n",
    "        if(loss > new_loss + alpha * t):\n",
    "            return newW, newU\n",
    "        else:\n",
    "            alpha = tao * alpha\n",
    "\n",
    "def fixOrthant(GW, weight_W, new_weight_W):\n",
    "    mask = (weight_W == 0) * np.sign(GW) + (weight_W != 0) * np.sign(weight_W)\n",
    "    mask = mask * new_weight_W > 0\n",
    "    return new_weight_W * mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "import random\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                format='%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s',\n",
    "                datefmt='%a, %d %b %Y %H:%M:%S',\n",
    "                filename='myapp.log',\n",
    "                filemode='w')\n",
    "\n",
    "class LSPLM:\n",
    "\n",
    "    def __init__(self,\n",
    "                 pieceNum = 12,\n",
    "                 iterNum = 100,\n",
    "                 intercept = True,\n",
    "                 memoryNum = 10,\n",
    "                 beta = 0.1,\n",
    "                 lamb = 0.1,\n",
    "                 terminate = True\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        :param feaNum:  特征数\n",
    "        :param classNum:    类别数\n",
    "        :param iterNum:\n",
    "        :param intercept:\n",
    "        :param memoryNum:\n",
    "        :param beta:\n",
    "        :param lamb:\n",
    "        :param u_stdev:\n",
    "        :param w_stdev:\n",
    "        \"\"\"\n",
    "        self.pieceNum = pieceNum\n",
    "        self.iterNum = iterNum\n",
    "        self.intercept = intercept\n",
    "        self.memoryNum = memoryNum\n",
    "        self.beta = beta\n",
    "        self.lamb = lamb\n",
    "        self.N = 0\n",
    "        self.p = 0\n",
    "        self._predict = np.vectorize(self._mlr)\n",
    "        self.terminate = terminate\n",
    "\n",
    "    def fit(self,X, y):\n",
    "        \"\"\"\n",
    "            训练ls-plm large scale piece-wise linear model\n",
    "        :param data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # np.random.seed(0)\n",
    "        N, p = X.shape\n",
    "        self.N = N\n",
    "        if self.intercept:\n",
    "            self.p = p + 1\n",
    "            pad = np.ones((N, p + 1))\n",
    "            pad[:,:-1] = X\n",
    "            X = pad\n",
    "            del pad\n",
    "        else:\n",
    "            self.p = p\n",
    "\n",
    "        it = 0\n",
    "        ## Intialization\n",
    "        weight_W = np.random.normal(0,0.1,(self.pieceNum, self.p))\n",
    "        weight_U = np.random.normal(0,0.1,(self.pieceNum, self.p))\n",
    "        sList_w = np.zeros((self.memoryNum, self.pieceNum, self.p))\n",
    "        sList_u = np.zeros((self.memoryNum, self.pieceNum, self.p))\n",
    "        yList_w = np.zeros((self.memoryNum, self.pieceNum, self.p))\n",
    "        yList_u = np.zeros((self.memoryNum, self.pieceNum, self.p))\n",
    "        loss_before = calLoss(X, y, weight_W, weight_U, self.lamb, self.beta)\n",
    "        GW, GU = sumCalDerivative(weight_W, weight_U, X, y)\n",
    "        LW, LU = virtualGradient(weight_W, weight_U, GW, GU,self.beta,self.lamb)\n",
    "        del GW,GU\n",
    "        # print(\"loss: %s\" % loss)\n",
    "        # print(\"gradient_w: is\")\n",
    "        # for w in weight_W:\n",
    "        #     print (w)\n",
    "        # print(\"gradient_u: is\")\n",
    "        # for u in weight_U:\n",
    "        #     print(u)\n",
    "        # self.firstLoss = loss\n",
    "        # self.lossList.append(loss)\n",
    "        #\n",
    "        print(loss_before)\n",
    "        while it < self.iterNum:\n",
    "            logging.info('===========iterator : %s =============' % it)\n",
    "            logging.info('===========loss : %s ==============' % loss_before)\n",
    "            print \"iter:%d, lossdd:%s\" % (it, loss_before)\n",
    "            start_time = time.time()\n",
    "            # 1. 计算虚梯度\n",
    "                #计算梯度\n",
    "            GW, GU = sumCalDerivative(weight_W, weight_U, X, y)\n",
    "            print(\"............\" + str(GW[0][0]))\n",
    "            newLW, newLU = virtualGradient(weight_W, weight_U, GW, GU,self.beta,self.lamb)\n",
    "            \n",
    "            print(newLW[0][0])\n",
    "            del GW,GU\n",
    "\n",
    "\n",
    "            # dirW = copy.deepcopy(vGW)\n",
    "            # dirU = copy.deepcopy(vGU)\n",
    "\n",
    "            # 3. 利用LBFGS算法的两个循环计算下降方向, 这里会直接修改vGradient, 并确定下降方向是否跨象限\n",
    "\n",
    "            PW, PU = lbfgs(newLW ,newLU, sList_w,sList_u, yList_w, yList_u, it, self.memoryNum, it % self.memoryNum)\n",
    "\n",
    "            # # 4. 确定下降方向是否跨象限， 这里也会直接修改vGradient\n",
    "            # fc.fixDirection(vG, dir)\n",
    "\n",
    "            # 5. 线性搜索最优解\n",
    "            # new_weight_W, new_weight_U = backTrackingLineSearch(X, y, weight_W, weight_U,self.lamb, self.beta, PW, PU)\n",
    "            new_weight_W, new_weight_U = weight_W + PW, weight_U + PU\n",
    "\n",
    "            del PW, PU\n",
    "\n",
    "            new_weight_W = fixOrthant(newLW, weight_W, new_weight_W)\n",
    "            new_weight_U = fixOrthant(newLU, weight_U, new_weight_U)\n",
    "            loss_now = calLoss(X, y, new_weight_W, new_weight_U, self.lamb, self.beta)\n",
    "\n",
    "\n",
    "                # 6. 判断是否提前终止\n",
    "            if self.terminate and self.check(loss_before, loss_now):\n",
    "                break\n",
    "            else:\n",
    "                overwrite = (it + 1) % self.memoryNum\n",
    "                yList_u[overwrite] = LU - newLU\n",
    "                yList_w[overwrite] = LW - newLW\n",
    "                sList_u[overwrite] = new_weight_U - weight_U\n",
    "                sList_w[overwrite] = new_weight_W - weight_W\n",
    "                weight_U = new_weight_U\n",
    "                weight_W = new_weight_W\n",
    "                LW = newLW\n",
    "                LU = newLU\n",
    "                del newLW\n",
    "                del newLU\n",
    "                del new_weight_U\n",
    "                del new_weight_W\n",
    "                loss_before = loss_now\n",
    "\n",
    "            it += 1\n",
    "\n",
    "\n",
    "        logging.info(\"============iterator : %s end ==========\" % it)\n",
    "        print(\"\")\n",
    "\n",
    "        print(\"use time: \", time.time() - start_time)\n",
    "        print(\"------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "\n",
    "        # with open(\"save/result\"+self.stamp,\"a\") as fw:\n",
    "        #     fw.write(\"train_acc:\" + \" \".join(ACC)+\"\\n\")\n",
    "        #     fw.write(\"train_loss:\" + \" \".join(LOSS) + \"\\n\")\n",
    "        #     fw.write(\"train_auc:\" + \" \".join(AUC) + \"\\n\")\n",
    "        #     fw.write(\"test_acc:\" + \" \".join(TEST_ACC) + \"\\n\")\n",
    "        #     fw.write(\"test_loss:\" + \" \".join(TEST_LOSS) + \"\\n\")\n",
    "        #     fw.write(\"test_auc:\" + \" \".join(TEST_AUC) + \"\\n\")\n",
    "        self.weight_W = weight_W\n",
    "        self.weight_U = weight_U\n",
    "    \n",
    "    def check(self, loss_before, loss_now):\n",
    "        return abs(loss_before - loss_now) / loss_now < 0.01\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self._predict(X)\n",
    "\n",
    "    def _mlr(self, x):\n",
    "        return mlr(self.weight_W,self.weight_U, (x, 1))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array(self._predict(X) > 0.5, dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1 = np.random.normal(1,0.2, (100, 10))\n",
    "X2 = np.random.normal(-1,0.2, (100,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = np.zeros(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,100):\n",
    "    Y[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.vstack((X1,X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 10)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls = LSPLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(138.99187425734931, 0.16881353702723154, 1.5287392694446333)\n",
      "140.689427064\n",
      "iter:0, lossdd:140.689427064\n",
      "............-11.3393598371\n",
      "11.4403156844\n",
      "(0.0090797798433644421, 6.2149230534891906, 50.343789580242202)\n",
      "iter:1, lossdd:56.5677924136\n",
      "............-2.87087148488e-05\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:2, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:3, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:4, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:5, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:6, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:7, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:8, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:9, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:10, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:11, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:12, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:13, lossdd:nan\n",
      "............nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:198: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/chris/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:302: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:14, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:15, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:16, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:17, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:18, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:19, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:20, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:21, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:22, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:23, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:24, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:25, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:26, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:27, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:28, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:29, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:30, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:31, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:32, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:33, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:34, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:35, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:36, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:37, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:38, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:39, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:40, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:41, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:42, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:43, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:44, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:45, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:46, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:47, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:48, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:49, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:50, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:51, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:52, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:53, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:54, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:55, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:56, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:57, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:58, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:59, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:60, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:61, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:62, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:63, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:64, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:65, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:66, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:67, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:68, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:69, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:70, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:71, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:72, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:73, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:74, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:75, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:76, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:77, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:78, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:79, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:80, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:81, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:82, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:83, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:84, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:85, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:86, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:87, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:88, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:89, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:90, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:91, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:92, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:93, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:94, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:95, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:96, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:97, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:98, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "iter:99, lossdd:nan\n",
      "............nan\n",
      "nan\n",
      "(nan, nan, nan)\n",
      "\n",
      "('use time: ', 0.01116490364074707)\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ls.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "       [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "       [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "       [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "       [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "       [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "       [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "       [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "       [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "       [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "       [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "       [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
