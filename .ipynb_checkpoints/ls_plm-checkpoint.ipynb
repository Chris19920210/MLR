{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from __future__ import division\n",
    "import traceback\n",
    "\n",
    "# with dense vector negative sample 0, postive sample 1\n",
    "def performance(f):\n",
    "    def fn(*args, **kw):\n",
    "        t_start = time.time()\n",
    "        r = f(*args, **kw)\n",
    "        t_end = time.time()\n",
    "        print ('call %s() in %fs' % (f.__name__, (t_end - t_start)))\n",
    "        return r\n",
    "    return fn\n",
    "\n",
    "\n",
    "# U is 2d-array with 2m*d dimension for each person item tuple of (x, label)\n",
    "\n",
    "def mlr(W, U, x):\n",
    "    \"\"\"\n",
    "    calculate mixture logistic regression\n",
    "    :param U: m * d\n",
    "    :param W: m * d\n",
    "    :param x: d\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    ux = np.dot(U, x)\n",
    "    eux = softmax(ux)\n",
    "    del ux\n",
    "    return np.dot(eux, sigmoid(np.dot(W, x)))\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    calculate sigmoid\n",
    "    :param z:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    softmax a array\n",
    "    :param x:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    e_x = np.exp(x)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def calLoss(X, y, weight_W, weight_U, norm21, norm1):\n",
    "    \"\"\"\n",
    "    :param data:\n",
    "    :param weight_W:\n",
    "    :param weight_U:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    functionLoss = calFunctionLoss(weight_W, weight_U, X, y)\n",
    "    norm21Loss = calNorm21(weight_W + weight_U)\n",
    "    norm1Loss = calNorm1(weight_W + weight_U)\n",
    "    print(functionLoss , norm21 * norm21Loss , norm1 * norm1Loss)\n",
    "    return functionLoss + norm21 * norm21Loss + norm1 * norm1Loss\n",
    "\n",
    "def calFunctionLossOne(W_w, W_u,x, y):\n",
    "    p = mlr(W_w, W_u, x)\n",
    "    if y == 0:\n",
    "        return - np.log(1 - p)\n",
    "    else:\n",
    "        return - np.log(p)\n",
    "\n",
    "\n",
    "def calFunctionLoss(W_w, W_u, X, y):\n",
    "    \"\"\"\n",
    "    calculate the loss over all data\n",
    "    :param w_w:\n",
    "    :param w_u:\n",
    "    :param data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    loss = map(lambda (x,y): calFunctionLossOne(W_w,W_u,x, y), zip(X, y))\n",
    "    loss = sum(loss)\n",
    "    return loss\n",
    "    # print(\"loss is:  %s\" % loss)\n",
    "\n",
    "def calNorm21(weight):\n",
    "    '''\n",
    "        计算norm21\n",
    "    :param weight:\n",
    "    :return:\n",
    "    '''\n",
    "    return (weight ** 2).sum() ** 0.5\n",
    "\n",
    "def calNorm1(weight):\n",
    "    \"\"\"\n",
    "        计算norm1\n",
    "    :param weight:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return np.abs(weight).sum()\n",
    "\n",
    "def calDimension21(W):\n",
    "    \"\"\"\n",
    "        计算每一个维度的L2\n",
    "    :param W:\n",
    "    :return:{dimension1:std1, dimension2:std2 ......}\n",
    "    \"\"\"\n",
    "    return (W**2).sum(axis = 0) ** 0.5\n",
    "\n",
    "# derivative for each sample\n",
    "def cal_derivative(W_w, W_u, x, y):\n",
    "    \"\"\"\n",
    "    calculate derivative\n",
    "    :param weight:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    ux = np.dot(W_u, x)\n",
    "    eux = softmax(ux)\n",
    "    del ux\n",
    "    sig = sigmoid(np.dot(W_w, x))\n",
    "    mlr = np.dot(eux, sig)\n",
    "    prob_scalar =  - (y - mlr) / (mlr * (1 - mlr))\n",
    "    dir_U = np.outer(prob_scalar * eux * (sig - mlr), x)\n",
    "    dir_W = np.outer(prob_scalar * sig * (1 - sig) * eux, x)\n",
    "    return dir_W, dir_U\n",
    "\n",
    "\n",
    "def sumCalDerivative(WW, WU, X, y):\n",
    "    all = map(lambda (x,y): cal_derivative(WW, WU,x, y), zip(X,y))\n",
    "    LW, LU = reduce(lambda x, y: (x[0] + y[0], x[1] + y[1]),all,(0,0))\n",
    "    return LW, LU\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def virtualGradient(WW, WU, GW, GU,beta,lamb):\n",
    "    \"\"\"\n",
    "    :param weight_W:\n",
    "    :param weight_U:\n",
    "    :param gradient_W:\n",
    "    :param gradient_U:\n",
    "    :param norm21:\n",
    "    :param norm1:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #计算θ_i·\n",
    "    D21 = calDimension21(WW + WU)\n",
    "    #计算v：\n",
    "    VW = calV(GW, beta)\n",
    "    VU = calV(GU, beta)\n",
    "    #计算v_i·\n",
    "    VD21 = calDimension21(VW + VU)\n",
    "    sumVD21 = sum(VD21)\n",
    "    #计算d_ij\n",
    "    DW = calDij(GW, WW, VW, D21, sumVD21, beta, lamb)\n",
    "    DU = calDij(GU, WU, VU, D21, sumVD21, beta, lamb)\n",
    "    return DW, DU\n",
    "\n",
    "\n",
    "def calV(L, beta):\n",
    "    \"\"\"\n",
    "    :param LW:\n",
    "    :param LU:\n",
    "    :param beta:\n",
    "    :param lamb:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    V = np.copy(L)\n",
    "    V = np.maximum(np.abs(V) - beta, 0)\n",
    "    return V*np.sign(-L)\n",
    "\n",
    "def calDij(L, W, V, D21, sumVD21, beta, lamb):\n",
    "    mask1 = (W != 0)\n",
    "    mask2 = (W == 0) * np.tile((D21 != 0), (len(W),1))\n",
    "    mask3 = np.tile((D21 == 0), (len(W),1))\n",
    "    D21_tmp = np.copy(D21)\n",
    "    D21_tmp[D21_tmp == 0] = 1\n",
    "    s = - L - lamb * W / D21_tmp\n",
    "    cond1 =  s - beta * np.sign(W)\n",
    "    cond2 = np.maximum(np.abs(s) - beta, 0.0)*np.sign(s)\n",
    "    if (sumVD21 != 0 ):\n",
    "        cond3 = V * (max(sumVD21 - lamb, 0.0) / sumVD21)\n",
    "    else:\n",
    "        cond3 = V * max(sumVD21 - lamb, 0.0)\n",
    "    return mask1 * cond1 + mask2 * cond2 + mask3 * cond3\n",
    "\n",
    "\n",
    "def loop(length, latest, direction):\n",
    "    count = 0\n",
    "    if(direction == 'right'):\n",
    "        while(count < length):\n",
    "            if(latest + count + 1 < length):\n",
    "                yield latest + count + 1\n",
    "                count += 1\n",
    "            else:\n",
    "                yield latest + count + 1 - length\n",
    "                count += 1\n",
    "    elif(direction == 'left'):\n",
    "        while(count < length):\n",
    "            if(latest - count >= 0):\n",
    "                yield latest - count\n",
    "                count += 1\n",
    "            else:\n",
    "                yield length + latest - count\n",
    "                count += 1\n",
    "    else:\n",
    "        raise Exception(\"please enter left or right\")\n",
    "\n",
    "\n",
    "def adam(VW, VU,m_w, m_u, v_w, v_u, beta1, beta2, it, alpha, epison):\n",
    "    m_w = beta1 * m_w - (1 - beta1) * VW\n",
    "    m_u = beta1 * m_u - (1 - beta1) * VU\n",
    "    v_w = beta2 * v_w + (1 - beta2) * (VW**2)\n",
    "    v_u = beta2 * v_u + (1 - beta2) * (VU**2)\n",
    "    m_w_hat = -m_w / (1 - beta1 ** it)\n",
    "    m_u_hat = -m_u / (1 - beta1 ** it)\n",
    "    mask_w = np.sign(m_w_hat) * np.sign(VW) > 0\n",
    "    mask_u = np.sign(m_u_hat) * np.sign(VU) > 0\n",
    "    m_w_hat = m_w_hat * mask_w\n",
    "    m_u_hat = m_u_hat * mask_u\n",
    "    v_w_hat = v_w / (1 - beta2**it)\n",
    "    v_u_hat = v_u / (1 - beta2**it)\n",
    "    return m_w, m_u, v_w, v_u, alpha * (m_w_hat / (v_w_hat ** 0.5 + epison)), alpha * (m_u_hat / (v_u_hat  ** 0.5 + epison))\n",
    "\n",
    "\n",
    "## weight_w, weight_u, s\n",
    "def lbfgs(VW, VU, sList_w,sList_u, yList_w, yList_u, k, m, start):\n",
    "    \"\"\"\n",
    "    :param feaNum:\n",
    "    :param gk : matrix, 2m*d\n",
    "    :param sList:3d*matrix,steps * 2m * d\n",
    "    :param yList:3d*matrix, steps * 2m * d\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if((sList_w[start] * yList_w[start] + sList_u[start] * yList_u[start]).sum() > 0 ):\n",
    "        q_u = np.copy(VU)\n",
    "        q_w = np.copy(VW)\n",
    "        # for delta\n",
    "        L = k + 1 if k < m else m\n",
    "        alphaList = np.zeros(L)\n",
    "        ro = (yList_w[(start - 1) % m] * sList_w[(start - 1) % m] + yList_u[(start - 1) % m] * sList_u[(start - 1) % m]).sum()\n",
    "        print (\"ro %f\" % ro)\n",
    "\n",
    "        for i in loop(L, start, 'left'):\n",
    "            alpha = (sList_u[i] * q_u + sList_w[i] * q_w).sum() / (sList_u[i] * yList_u[i] + sList_w[i] * yList_u[i]).sum()\n",
    "            print (\"alpha %f\" % alpha)\n",
    "            if(alpha == np.nan or alpha == np.inf or alpha == -np.inf):\n",
    "                return VW, VU\n",
    "            q_u = q_u - alpha * yList_u[i]\n",
    "            q_w = q_w - alpha * yList_w[i]\n",
    "            alphaList[i] = alpha\n",
    "        \n",
    "        q_u = q_u * ro\n",
    "        q_w = q_w * ro\n",
    "\n",
    "        for i in loop(L,start,'right'):\n",
    "            beta = (yList_u[i] * q_u + yList_w[i] * q_w).sum() / (sList_u[i] * yList_u[i] + sList_w[i] * yList_u[i]).sum()\n",
    "            q_u = q_u + (alphaList[i] - beta) * sList_u[i]\n",
    "            q_w = q_w + (alphaList[i] - beta) * sList_w[i]\n",
    "\n",
    "        mask_u = np.sign(q_u) * np.sign(VU) > 0\n",
    "        mask_w = np.sign(q_w) * np.sign(VW) > 0\n",
    "\n",
    "        return q_w * mask_w, q_u * mask_u\n",
    "    else:\n",
    "        return VW, VU\n",
    "\n",
    "def backTrackingLineSearch(X, y, weight_W, weight_U,norm21, norm1, pW, pU):\n",
    "    \"\"\"\n",
    "    :param it:\n",
    "    :param oldLoss:\n",
    "    :param data:\n",
    "    :param WW:\n",
    "    :param WU:\n",
    "    :param GW:\n",
    "    :param GU:\n",
    "    :param vGW:\n",
    "    :param vGU:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    alpha = 1.0\n",
    "    c = 0.5\n",
    "    tao = 0.9\n",
    "    LW, LU = sumCalDerivative(weight_W, weight_U, X, y)\n",
    "    m = (pW * LW + pU * LU).sum()\n",
    "    t = - c * m\n",
    "    loss = calLoss(X, y, weight_W, weight_U, norm21, norm1)\n",
    "\n",
    "    while True:\n",
    "        newW = weight_W - alpha*pW\n",
    "        newU = weight_U - alpha*pU\n",
    "\n",
    "        new_loss = calLoss(X, y, newW, newU, norm21, norm1)\n",
    "\n",
    "        if(loss > new_loss + alpha * t):\n",
    "            return newW, newU\n",
    "        else:\n",
    "            alpha = tao * alpha\n",
    "\n",
    "def fixOrthant(GW, weight_W, new_weight_W):\n",
    "    mask = (weight_W == 0) * np.sign(GW) + (weight_W != 0) * np.sign(weight_W)\n",
    "    mask = mask * new_weight_W > 0\n",
    "    return new_weight_W * mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "import random\n",
    "import logging\n",
    "from function import *\n",
    "\n",
    "class LSPLM:\n",
    "\n",
    "    def __init__(self,\n",
    "                 pieceNum = 12,\n",
    "                 iterNum = 1000,\n",
    "                 intercept = True,\n",
    "                 beta1 = 0.1,\n",
    "                 beta2 = 0.1,\n",
    "                 alpha = 0.001,\n",
    "                 epison = 10e-8,\n",
    "                 lamb = 0.1,\n",
    "                 beta = 0.1,\n",
    "                 terminate = False\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        :param feaNum:  特征数\n",
    "        :param classNum:    类别数\n",
    "        :param iterNum:\n",
    "        :param intercept:\n",
    "        :param memoryNum:\n",
    "        :param beta:\n",
    "        :param lamb:\n",
    "        :param u_stdev:\n",
    "        :param w_stdev:\n",
    "        \"\"\"\n",
    "        self.pieceNum = pieceNum\n",
    "        self.iterNum = iterNum\n",
    "        self.intercept = intercept\n",
    "        self.beta = beta\n",
    "        self.lamb = lamb\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.alpha = alpha\n",
    "        self.N = 0\n",
    "        self.p = 0\n",
    "        self.terminate = terminate\n",
    "        self.epison = epison\n",
    "\n",
    "    def fit(self,X, y):\n",
    "        \"\"\"\n",
    "            训练ls-plm large scale piece-wise linear model\n",
    "        :param data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # np.random.seed(0)\n",
    "        N, p = X.shape\n",
    "        self.N = N\n",
    "        if self.intercept:\n",
    "            self.p = p + 1\n",
    "            pad = np.ones((N, p + 1))\n",
    "            pad[:,:-1] = X\n",
    "            X = pad\n",
    "            del pad\n",
    "        else:\n",
    "            self.p = p\n",
    "\n",
    "        ## Intialization\n",
    "        np.random.seed(0)\n",
    "        weight_W = np.random.normal(0,0.1, (self.pieceNum, self.p))\n",
    "        weight_U = np.random.normal(0,0.1, (self.pieceNum, self.p))\n",
    "        best_weight_W = np.random.normal(0,0.1, (self.pieceNum, self.p))\n",
    "        best_weight_U = np.random.normal(0,0.1, (self.pieceNum, self.p))\n",
    "        m_w = np.zeros((self.pieceNum, self.p))\n",
    "        m_u = np.zeros((self.pieceNum, self.p))\n",
    "        v_w = np.zeros((self.pieceNum, self.p))\n",
    "        v_u = np.zeros((self.pieceNum, self.p))\n",
    "        loss_before = calLoss(X, y, weight_W, weight_U, self.lamb, self.beta)\n",
    "        GW, GU = sumCalDerivative(weight_W, weight_U, X, y)\n",
    "        LW, LU = virtualGradient(weight_W, weight_U, GW, GU,self.beta,self.lamb)\n",
    "        it = 1\n",
    "        del GW,GU\n",
    "        loss_best = np.maximum\n",
    "        best_iter = 0\n",
    "        optimize_counter = 0\n",
    "        while it < self.iterNum:\n",
    "            print \"iter:%d, loss:%s\" % (it, loss_before)\n",
    "            start_time = time.time()\n",
    "            \n",
    "            GW, GU = sumCalDerivative(weight_W, weight_U, X, y)\n",
    "            newLW, newLU = virtualGradient(weight_W, weight_U, GW, GU,self.beta,self.lamb)\n",
    "            del GW,GU\n",
    "\n",
    "\n",
    "            PW, PU = adam(newLW, newLU, m_w, m_u, v_w, v_u, self.beta1, self.beta2, it, self.alpha, self.epison)\n",
    "\n",
    "            new_weight_W, new_weight_U = weight_W + PW, weight_U + PU\n",
    "\n",
    "            del PW, PU\n",
    "\n",
    "            new_weight_W = fixOrthant(newLW, weight_W, new_weight_W)\n",
    "            new_weight_U = fixOrthant(newLU, weight_U, new_weight_U)\n",
    "            loss_now = calLoss(X, y, new_weight_W, new_weight_U, self.lamb, self.beta)\n",
    "            if(loss_now < loss_best):\n",
    "                loss_best = loss_now\n",
    "                best_iter = it\n",
    "                best_weight_W = new_weight_W\n",
    "                best_weight_U = new_weight_U\n",
    "            if(loss_before < loss_now):\n",
    "                optimize_counter += 1\n",
    "                if(optimize_counter >= 10):\n",
    "                    self.weight_U = best_weight_U\n",
    "                    self.weight_W = best_weight_W\n",
    "                    print(\"use time: \", time.time() - start_time)\n",
    "                    print(\"The best result get at %d iteration with loss: %f\" % (best_iter, loss_best))\n",
    "                    return \"Done!\"\n",
    "            else:\n",
    "                optimize_counter = 0\n",
    "            \n",
    "\n",
    "            weight_U = new_weight_U\n",
    "            weight_W = new_weight_W\n",
    "            LW = newLW\n",
    "            LU = newLU\n",
    "            del newLW\n",
    "            del newLU\n",
    "            del new_weight_U\n",
    "            del new_weight_W\n",
    "            loss_before = loss_now\n",
    "            \n",
    "\n",
    "            it += 1\n",
    "\n",
    "\n",
    "        logging.info(\"============iterator : %s end ==========\" % it)\n",
    "        print(\"\")\n",
    "\n",
    "        print(\"The best result get at %d iteration with loss: %f\" % (best_iter, loss_best))\n",
    "        print(\"Done!\")\n",
    "        self.weight_W = best_weight_W\n",
    "        self.weight_U = best_weight_U\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        N, p = X.shape\n",
    "        if self.intercept:\n",
    "            pad = np.ones((N, p + 1))\n",
    "            pad[:,:-1] = X\n",
    "            X = pad\n",
    "            del pad\n",
    "        return np.array(map(lambda x: mlr(self.weight_W,self.weight_U,x),X))\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array(self.predict_proba(X) > 0.5, dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-081b5f1bbba3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSPLM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterNum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlamb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpieceNum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# test\n",
    "X1 = np.random.normal(1,0.2, (100, 10))\n",
    "X2 = np.random.normal(-1,0.2, (100,10))\n",
    "X = \n",
    "y = np.zeros(200)\n",
    "for i in range(0,100):\n",
    "    y[i] = 1\n",
    "ls = LSPLM(iterNum=1000,lamb = 2,beta = 2,pieceNum=2)\n",
    "ls.fit(X,Y)\n",
    "y_test = ls.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-270dd209b8a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
